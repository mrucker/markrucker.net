---
layout: post
---
Machine Learning is perhaps one of the most important idea of our time. It shapes our lives  (e.g., Google Search) and our dreams (e.g. The Matrix). New descriptive words have entered our vernacular to try and grapple with these tools. Unfortunately, these words often sound more like they belong in ancient books of magic and mysticism than in our modern age: words like deep, neural and emergent. Fortunately, there is a simple solution. All machine learning algorithms can be boiled down to one equation, Y = f(X) + e, and consequently a simple vocabulary that uses everyday ideas we are all familiar with.

To begin, we'll translate Y = f(X) + e (which is written in Algebra) into English (i.e., math is a language and, like any other language, can be translated). First, we'll replace X and Y to get Results = f(Inputs) + e. We still haven't translated "f" or "e", but we can already start to make sense of Machine Learning. No matter the method, all learning algorithms are simply about taking a set of Inputs and somehow turning them into Results.

Now, we'll translate "f" and "e" into "Action" and "Chance", respectively, and clean up those parentheses a bit. This now gives us Results = Action + Inputs + Chance. This is still a little muddy, but before we clean it up a little more I want to point out something really important here. Chance is an integral component of all learning algorithms. No machine learning algorithm will ever be all knowing. There will always be a chance they are wrong.

Now that we've roughly translated our original equation let's explore it a little more. The thing that I find most interesting about this equation is that it is really quite banal. For example, let's replace our generic words with a specific example. (Drop + Ball + Chance) = (Ball Falls or Doesn't Fall). And that's it, according to our "Machine Learning" equation, if I drop a ball, it will either fall or not fall. We can probably say that 99.999% of the time the ball will fall, but once in a blue moon chance might have it that a bird grabs our ball and flies off the instant we drop it.

The second thing I find interesting about this equation is that we can be add specificity as an after thought using our equation. For example, our ball drop example isn't very specific. Let's say we want to be more specific about what Drop means. We could then define it using another equation (Hand Releases + Object In Hand + Chance) = Drop. Now, we know that Drop means something I drop from my hand rather than by some other means.

And that's it. We've just explained how all machine learning algorithms work using only the ideas of Results, Action, Inputs and Chance. No need for "neural nets" or "deep learning" or "oracles". Still, those words have some meaning. In the case of Machine Learning they refer to the different kinds of "Actions". So the equation for a Neural Net would look like this, Results = Neural-Net(Inputs) + Chance. While the equation for a Deep Learning algorithm would be Results = Deep-Learning(Inputs) + Chance.

(For even more information than is contained here I recommend chapter 2 of [Introduction To Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf))  

